# AI 알고리즘 편향성 완화 실험 - 편향 정도별 성능 측정

## 실험 개요

이 프로젝트는 **편향이 낮은 데이터셋에서도 판별·완화 성능을 측정**하는 혁신적인 실험입니다. 다양한 편향 정도의 데이터셋을 생성하고, 편향 판별 모델의 민감도와 완화 기법의 효과를 체계적으로 분석합니다.

### 실험 목적
- **편향이 낮은 데이터셋에서도 편향 판별 모델의 성능 검증**
- 다양한 편향 정도(낮음/보통/높음)에서의 편향 감지 민감도 측정
- 편향 완화 기법의 효과를 정량적으로 비교 및 분석
- 정확도와 공정성의 트레이드오프 관계 분석
- **거버넌스 정책 수립을 위한 실증 데이터 제공**

### 산출물
1. **편향 정도별 데이터셋** (낮음/보통/높음)
2. **편향 판별 모델의 민감도 곡선** (ROC 유사)
3. **편향 완화 기법별 성능 비교** (정확도 vs 공정성)
4. **거버넌스 정책 권고사항** 및 리스크 지표
5. 재현 가능한 코드 및 데이터 설명

## 데이터셋

**3개 균형 잡힌 데이터셋 - 편향 정도별 버전**

### 1. FairFace (인종/성별/연령 균형)
- **예측 태스크**: 고소득 여부 분류
- **민감 속성**: 성별, 인종, 연령
- **샘플 수**: 10,000개
- **특징**: 7개 인종 카테고리, 2개 성별, 9개 연령대 균등 분포

#### FairFace 데이터셋 사용 방법
```bash
# 1. FairFace 데이터셋 다운로드
git clone https://github.com/joojs/fairface.git
cd fairface

# 2. 데이터 다운로드 (공식 링크에서)
# https://github.com/dchen236/FairFace

# 3. 데이터 구조
fairface/
├── labels.csv          # 인종, 성별, 연령 라벨
├── images/             # 얼굴 이미지 파일들
└── README.md           # 데이터셋 설명
```

**참고**: FairFace는 [WACV 2021](https://openaccess.thecvf.com/content/WACV2021/papers/Karkkainen_FairFace_Face_Attribute_Dataset_for_Balanced_Race_Gender_and_Age_WACV_2021_paper.pdf)에서 발표된 편향 완화 연구에 특화된 데이터셋입니다.

### 2. BFW (Balanced Faces in the Wild)
- **예측 태스크**: 고학력 여부 분류  
- **민감 속성**: 성별, 인종
- **샘플 수**: 8,000개
- **특징**: 4개 인종 카테고리와 성별 균등 분포

### 3. Common Voice Balanced Subset (성별/연령 균형 음성)
- **예측 태스크**: 음성 품질 우수 여부 분류
- **민감 속성**: 성별, 연령대
- **샘플 수**: 12,000개
- **특징**: 5개 연령대와 성별 균등 분포

### 편향 정도별 데이터셋 생성
각 데이터셋에 대해 3단계 버전 생성:
1. **원본 그대로**: 편향 낮음 (baseline)
2. **약간 비율 왜곡**: 편향 소폭 존재 (15% 증가, 10% 감소)
3. **심하게 왜곡**: 편향 높음 (40% 증가, 30% 감소)

**핵심**: **편향이 낮은 데이터셋에서도 편향 판별·완화 성능을 검증**하는 혁신적 접근

## 편향 측정 지표

1. **Statistical Parity Difference (SPD)**: 예측 긍정률 차이
2. **Equal Opportunity Difference (EOD)**: TPR 차이
3. **Disparate Impact (DI)**: 4/5 Rule (0.8~1.25 범위 권장)
4. **Average Odds Difference (AOD)**: FPR/TPR 평균 차이

## 편향 완화 기법

### 학습 전 (Pre-processing)
- **Reweighing**: 데이터 샘플 가중치 재조정으로 편향 완화

### 학습 중 (In-processing)
- **Adversarial Debiasing**: 편향 예측기를 둬서 모델이 민감 속성을 구별 못하게 학습

### 학습 후 (Post-processing)
- **Threshold Adjustment**: Fairlearn의 ThresholdOptimizer를 사용한 예측 결과 조정

**핵심**: 각 기법을 **편향 정도별 데이터셋**에 적용하여 효과를 체계적으로 비교

## 설치 및 실행

### 1. 환경 설정

```bash
# 필요한 패키지 설치
pip install -r requirements.txt
```

### 2. 실험 실행

```bash
# Python 스크립트 실행
python ai_bias_mitigation_experiment.py
```

### 3. Google Colab에서 실행

1. `ai_bias_mitigation_experiment.py` 파일을 Colab에 업로드
2. 필요한 패키지 설치:
   ```python
   !pip install aif360 fairlearn scikit-learn pandas numpy matplotlib seaborn
   ```
3. 스크립트 실행

## 실험 구조

```
1. 3개 데이터셋 로딩 및 전처리
   - FairFace (인종/성별 균형)
   - BFW (인종/성별 균형)  
   - Common Voice (성별/연령 균형)
   ↓
2. 각 데이터셋별 편향 정도별 버전 생성 (낮음/보통/높음)
   ↓
3. 각 데이터셋별 편향 판별 모델 민감도 측정
   - 각 편향 정도에서의 판별 성능
   - 민감도 곡선(ROC 유사) 생성
   ↓
4. 각 데이터셋별 편향 완화 기법 적용 및 성능 비교
   - Reweighing
   - Adversarial Debiasing
   - Threshold Adjustment
   ↓
5. 3개 데이터셋 통합 편향 판별 민감도 곡선 생성
   ↓
6. 3개 데이터셋 통합 편향 완화 기법별 성능 비교 시각화
   ↓
7. 통합 거버넌스 제안 및 정책 권고사항 생성
   ↓
8. 실험 결과 요약 및 분석
```

## 결과 분석

### 주요 그래프
1. **편향 판별 민감도 곡선**: 각 편향 정도에서의 판별 성능 (ROC 유사)
2. **편향 완화 기법별 성능 비교**: 정확도, SPD 등 지표 비교
3. **정확도 vs 공정성 트레이드오프**: 두 지표 간의 관계 분석
4. **편향 정도별 완화 효과**: 각 기법의 편향 감소율 분석

### 핵심 분석 포인트
- **편향이 낮은 데이터셋에서도 편향 판별 모델이 얼마나 민감하게 반응하는가?**
- **각 편향 완화 기법이 다양한 편향 정도에서 어떤 효과를 보이는가?**
- **3개 데이터셋(이미지, 음성)에서 편향 완화 기법의 일관성은 어떠한가?**
- **정확도와 공정성의 최적 균형점은 어디인가?**

### 핵심 결론 도출
- **"편향이 낮은 데이터셋에서도 편향 판별·완화 성능이 검증되었는가?"**
- **"각 편향 완화 기법이 다양한 편향 정도에서 일관된 효과를 보이는가?"**
- **"3개 데이터셋(이미지, 음성)에서 편향 완화 기법의 크로스 도메인 일관성은 어떠한가?"**
- 각 기법별 장단점 분석 및 최적 기법 선정
- **거버넌스 정책 수립을 위한 종합적 실증 데이터 제공**

## 확장 아이디어

### 1. **거버넌스 정책 연결**
- **"편향이 낮은 데이터셋에서도 판별·완화 성능 검증"을 모델 평가 의무 항목으로 포함**
- **과도 교정/미교정에 대한 리스크 지표 설정**
- AI 편향성 위원회 구성 및 정기 감사 체계

### 2. **실시간 편향 모니터링**
- 다양한 편향 정도에서의 임계값 설정
- 편향 감지 시 자동 완화 기법 적용
- 편향 완화 효과의 실시간 추적

### 3. **규제 준수 시뮬레이션**
- 국내 규제안 (AI 기본법) 적용
- 해외 규범 (EU AI Act) 비교
- **편향 완화 성능 검증 의무화 방안**

## 기술 스택

- **Python**: scikit-learn, pandas, numpy
- **공정성 라이브러리**: AIF360, Fairlearn
- **시각화**: matplotlib, seaborn
- **환경**: Google Colab (GPU 불필요)

## 파일 구조

```
BLOOM/
├── ai_bias_mitigation_experiment.py  # 메인 실험 코드
├── requirements.txt                   # 필요한 패키지 목록
└── README.md                         # 이 파일
```

## 주의사항

1. **라이브러리 의존성**: AIF360과 Fairlearn이 설치되지 않으면 일부 기능이 제한됩니다.
2. **데이터 로딩**: 인터넷 연결이 필요하며, UCI 데이터셋에 접근할 수 있어야 합니다.
3. **메모리 사용량**: 대용량 데이터셋 처리 시 충분한 메모리가 필요합니다.

## 공모전 활용 방안

### 1. **혁신적 실험 설계**
- **"편향이 낮은 데이터셋에서도 판별·완화 성능 검증"이라는 독창적 접근**
- 구체적인 수치와 그래프로 편향 완화 효과 입증
- 재현 가능한 실험 설계

### 2. **기술적 완성도**
- 실제 구현 가능한 코드
- 체계적인 실험 설계 및 분석
- **편향 판별 민감도 곡선 등 고급 시각화**

### 3. **정책 연계성**
- **거버넌스 정책 수립을 위한 실증 데이터 제공**
- AI 편향성 규제 준수 방안 제시
- **실용적이고 확장 가능한 솔루션**

## 문의 및 지원

실험 실행 중 문제가 발생하거나 추가 기능이 필요한 경우, 코드 내 주석을 참고하거나 이슈를 등록해 주세요.

---

## 핵심 기여 및 혁신성

### 🎯 **핵심 기여**
- **"편향이 낮은 데이터셋에서도 판별·완화 성능 검증"이라는 독창적 접근**
- 다양한 편향 정도에서의 체계적 성능 측정
- 거버넌스 정책 수립을 위한 실증 데이터 제공

### 🚀 **혁신성**
- 편향 판별 모델의 민감도 곡선 생성 (ROC 유사)
- 편향 완화 기법의 일관성 검증
- 정책 수립을 위한 정량적 분석 방법론 제시

**이 실험을 통해 AI 시스템의 공정성을 보장하는 구체적이고 실용적인 방법론을 제시할 수 있습니다!**
