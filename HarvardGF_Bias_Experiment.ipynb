{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Harvard-GF 데이터셋 기반 의료 AI 편향성 분석\n",
        "\n",
        "이 노트북은 **Harvard Glaucoma Fairness (Harvard-GF)** 데이터셋을 사용하여, 녹내장 진단 AI 모델의 인종 간 편향성을 심층적으로 분석하는 과정을 담고 있습니다.\n",
        "\n",
        "FairFace 분석 노트북(`AIBiasExperiment.ipynb`)을 템플릿으로 삼아, 의료 영상 데이터셋의 특성에 맞게 데이터 로더와 일부 설정을 수정하였습니다.\n",
        "\n",
        "**실험 목표:**\n",
        "1.  **Baseline 모델 구축**: 안구 fundus 이미지로 CNN 모델(ResNet18)을 학습하고, 녹내장 진단에 대한 기본적인 성능과 인종 그룹별 공정성 지표를 측정합니다.\n",
        "2.  **Micro-Bias Sensitivity Curve 분석**: 데이터셋의 인종 그룹 비율을 미세하게 조정했을 때, 모델의 진단 성능이 얼마나 민감하게 변하는지 측정합니다.\n",
        "3.  **Over-Correction Damage Index (ODI) 계산**: 편향 완화 기법(Reweighing) 적용 시 발생하는 성능 저하(Trade-off)를 정량적으로 분석합니다.\n",
        "4.  **Hidden Subgroup Discovery**: 특정 인종과 다른 임상 정보(e.g., 연령대)가 조합된 하위집단에서 발생하는 잠재적 편향을 탐지합니다.\n",
        "\n",
        "---\n",
        "\n",
        "## 단계 1: 실험 환경 설정 및 데이터셋 준비\n",
        "\n",
        "의료 영상 처리에 필요한 라이브러리를 임포트하고, Harvard-GF 데이터셋을 불러올 준비를 합니다. 이미지를 모델이 학습할 수 있는 형태(Tensor)로 변환하는 PyTorch `Dataset`과 `DataLoader`를 정의합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 1.1 라이브러리 임포트 ===\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "import copy\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, Sampler\n",
        "from torchvision import transforms, models\n",
        "from torchvision.models import ResNet18_Weights\n",
        "from PIL import Image\n",
        "from collections import defaultdict\n",
        "\n",
        "# 경고 메시지 무시\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Matplotlib 스타일 설정\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "\n",
        "# === 1.2 주요 설정 (Configuration) ===\n",
        "# TODO: README에 따라 Harvard-GF 데이터셋을 'data/harvard_gf/' 경로에 준비해주세요.\n",
        "DATA_DIR = './data/harvard_gf/images/' # 실제 이미지 폴더 경로\n",
        "LABEL_FILE = './data/harvard_gf/labels.csv' # 실제 라벨 파일 경로\n",
        "\n",
        "# 실험 속성\n",
        "TARGET_ATTR = 'glaucoma'  # 예측 대상: 녹내장 유무 (Binary)\n",
        "SENSITIVE_ATTR = 'race' # 민감 속성\n",
        "\n",
        "# 모델 및 학습 하이퍼파라미터\n",
        "BATCH_SIZE = 32 # 의료 이미지는 크기가 클 수 있으므로 배치 사이즈 조정\n",
        "NUM_WORKERS = 0\n",
        "NUM_EPOCHS = 15 # 의료 데이터는 더 많은 학습이 필요할 수 있음\n",
        "LEARNING_RATE = 0.001\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# 파일 저장 경로\n",
        "MODEL_SAVE_PATH = 'best_harvardgf_model.pth'\n",
        "RESULTS_DIR = 'results_harvardgf'\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"Device: {DEVICE}\")\n",
        "print(f\"Target Attribute: {TARGET_ATTR}\")\n",
        "print(f\"Sensitive Attribute: {SENSITIVE_ATTR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 1.3 Harvard-GF 데이터셋 클래스 정의 ===\n",
        "\n",
        "class HarvardGFDataset(Dataset):\n",
        "    \"\"\"Harvard-GF 의료 이미지 데이터셋을 위한 PyTorch Dataset 클래스\"\"\"\n",
        "    \n",
        "    def __init__(self, data_dir, label_file, transform=None, indices=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        \n",
        "        try:\n",
        "            full_df = pd.read_csv(label_file)\n",
        "            # 제공된 인덱스가 있으면 해당 부분만 사용 (train/val 분할용)\n",
        "            self.labels_df = full_df.iloc[indices].reset_index(drop=True) if indices is not None else full_df\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Error: Label file not found at {label_file}\")\n",
        "            self.labels_df = pd.DataFrame()\n",
        "            return\n",
        "        \n",
        "        # TODO: 실제 라벨 파일의 컬럼명 확인 필요\n",
        "        # 예: 'image_filename', 'glaucoma_diagnosis', 'patient_race' 등\n",
        "        self.image_col = 'filename' \n",
        "        self.target_col = 'glaucoma'\n",
        "        self.sensitive_col = 'race'\n",
        "        \n",
        "        # 라벨 인코딩\n",
        "        self.glaucoma_map = {label: i for i, label in enumerate(sorted(self.labels_df[self.target_col].unique()))}\n",
        "        self.race_map = {label: i for i, label in enumerate(sorted(self.labels_df[self.sensitive_col].unique()))}\n",
        "        self.idx_to_glaucoma = {v: k for k, v in self.glaucoma_map.items()}\n",
        "        self.idx_to_race = {v: k for k, v in self.race_map.items()}\n",
        "\n",
        "        print(f\"Dataset loaded. Number of samples: {len(self.labels_df)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.data_dir, self.labels_df.loc[idx, self.image_col])\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "        except FileNotFoundError:\n",
        "            return self.__getitem__((idx + 1) % len(self))\n",
        "            \n",
        "        glaucoma = self.glaucoma_map[self.labels_df.loc[idx, self.target_col]]\n",
        "        race = self.race_map[self.labels_df.loc[idx, self.sensitive_col]]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "            \n",
        "        sample = {\n",
        "            'image': image, \n",
        "            'glaucoma': torch.tensor(glaucoma, dtype=torch.long), \n",
        "            'race': torch.tensor(race, dtype=torch.long)\n",
        "        }\n",
        "        return sample\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 1.4 데이터 변환, 분할 및 데이터로더 생성 ===\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(15),\n",
        "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ]),\n",
        "}\n",
        "\n",
        "try:\n",
        "    full_df = pd.read_csv(LABEL_FILE)\n",
        "    # train/val 분할\n",
        "    train_indices, val_indices = train_test_split(\n",
        "        range(len(full_df)),\n",
        "        test_size=0.2,\n",
        "        random_state=42,\n",
        "        stratify=full_df[TARGET_ATTR]\n",
        "    )\n",
        "\n",
        "    image_datasets = {\n",
        "        'train': HarvardGFDataset(data_dir=DATA_DIR, label_file=LABEL_FILE, transform=data_transforms['train'], indices=train_indices),\n",
        "        'val': HarvardGFDataset(data_dir=DATA_DIR, label_file=LABEL_FILE, transform=data_transforms['val'], indices=val_indices)\n",
        "    }\n",
        "\n",
        "    dataloaders = {\n",
        "        x: DataLoader(image_datasets[x], batch_size=BATCH_SIZE, shuffle=True if x == 'train' else False, num_workers=NUM_WORKERS)\n",
        "        for x in ['train', 'val']\n",
        "    }\n",
        "    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "\n",
        "    print(f\"DataLoaders created. Train size: {dataset_sizes['train']}, Val size: {dataset_sizes['val']}\")\n",
        "    \n",
        "    label_maps = {\n",
        "        'glaucoma': image_datasets['train'].glaucoma_map,\n",
        "        'race': image_datasets['train'].race_map\n",
        "    }\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "    print(\"Please check file paths and column names in the configuration cell.\")\n",
        "    dataloaders = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 단계 2: Baseline 모델 구축 및 학습\n",
        "\n",
        "Harvard-GF 데이터셋 준비가 완료되었으니, 사전 학습된 ResNet18 모델을 기반으로 녹내장 진단 Baseline 모델을 구축하고 fine-tuning을 진행합니다. 이 과정은 FairFace 분석과 동일한 모델 구조와 학습 방식을 사용합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 2.1 모델 정의 및 학습/평가 함수 ===\n",
        "\n",
        "def get_model(num_classes, pretrained=True):\n",
        "    weights = ResNet18_Weights.IMAGENET1K_V1 if pretrained else None\n",
        "    model = models.resnet18(weights=weights)\n",
        "    if pretrained:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "    num_ftrs = model.fc.in_features\n",
        "    model.fc = nn.Linear(num_ftrs, num_classes)\n",
        "    return model.to(DEVICE)\n",
        "\n",
        "def train_model(model, criterion, optimizer, dataloaders, num_epochs=10):\n",
        "    since = time.time()\n",
        "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch + 1}/{num_epochs}'); print('-' * 10)\n",
        "        for phase in ['train', 'val']:\n",
        "            model.train() if phase == 'train' else model.eval()\n",
        "            running_loss, running_corrects = 0.0, 0\n",
        "            for batch in tqdm(dataloaders[phase], desc=f\"{phase.capitalize()}\"):\n",
        "                inputs = batch['image'].to(DEVICE)\n",
        "                labels = batch[TARGET_ATTR].to(DEVICE)\n",
        "                optimizer.zero_grad()\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    if phase == 'train':\n",
        "                        loss.backward(); optimizer.step()\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "            \n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "            print(f'{phase.capitalize()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "            history[f'{phase}_loss'].append(epoch_loss)\n",
        "            history[f'{phase}_acc'].append(epoch_acc.item())\n",
        "\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
        "                print(f\"** Best val acc updated: {best_acc:.4f}, Model saved.\")\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'\\nTraining complete in {time_elapsed//60:.0f}m {time_elapsed%60:.0f}s')\n",
        "    print(f'Best val Acc: {best_acc:4f}')\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, history\n",
        "\n",
        "def plot_training_history(history):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "    ax1.plot(history['train_loss'], label='Train Loss'); ax1.plot(history['val_loss'], label='Val Loss')\n",
        "    ax1.set_title('Model Loss'); ax1.set_xlabel('Epochs'); ax1.set_ylabel('Loss'); ax1.legend()\n",
        "    ax2.plot(history['train_acc'], label='Train Acc'); ax2.plot(history['val_acc'], label='Val Acc')\n",
        "    ax2.set_title('Model Accuracy'); ax2.set_xlabel('Epochs'); ax2.set_ylabel('Accuracy'); ax2.legend()\n",
        "    fig.tight_layout()\n",
        "    plt.savefig(os.path.join(RESULTS_DIR, 'harvardgf_baseline_training_history.png'))\n",
        "    plt.show()\n",
        "\n",
        "print(\"Model and helper functions are defined.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 2.2 Baseline 모델 학습 실행 ===\n",
        "\n",
        "if dataloaders:\n",
        "    # 녹내장 유무는 Binary classification (2 classes)\n",
        "    NUM_CLASSES = len(label_maps[TARGET_ATTR])\n",
        "    baseline_model = get_model(num_classes=NUM_CLASSES, pretrained=True)\n",
        "    \n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(baseline_model.fc.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
        "\n",
        "    baseline_model, history = train_model(baseline_model, criterion, optimizer, dataloaders, num_epochs=NUM_EPOCHS)\n",
        "    \n",
        "    plot_training_history(history)\n",
        "else:\n",
        "    print(\"Dataloaders are not available. Cannot start training.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### 단계 2.3: Baseline 모델 성능 및 공정성 평가\n",
        "\n",
        "학습된 녹내장 진단 모델의 전반적인 성능과 인종 그룹별 공정성 지표를 평가합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 2.3 성능 및 공정성 평가 함수 정의 및 실행 ===\n",
        "\n",
        "def evaluate_fairness(model, dataloader, criterion):\n",
        "    model.eval()\n",
        "    y_true, y_pred, sensitive_attrs = [], [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            inputs = batch['image'].to(DEVICE)\n",
        "            labels = batch[TARGET_ATTR].to(DEVICE)\n",
        "            s_attrs = batch[SENSITIVE_ATTR].to(DEVICE)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(preds.cpu().numpy())\n",
        "            sensitive_attrs.extend(s_attrs.cpu().numpy())\n",
        "\n",
        "    y_true, y_pred, sensitive_attrs = np.array(y_true), np.array(y_pred), np.array(sensitive_attrs)\n",
        "    results = {'overall_acc': np.mean(y_true == y_pred)}\n",
        "    \n",
        "    group_accuracies, group_sizes = {}, {}\n",
        "    s_map = dataloader.dataset.idx_to_race\n",
        "    for group_idx, group_name in s_map.items():\n",
        "        mask = (sensitive_attrs == group_idx)\n",
        "        if mask.sum() > 0:\n",
        "            group_accuracies[group_name] = np.mean(y_true[mask] == y_pred[mask])\n",
        "            group_sizes[group_name] = mask.sum()\n",
        "    \n",
        "    results['group_accuracies'] = group_accuracies\n",
        "    results['group_sizes'] = group_sizes\n",
        "    if group_accuracies:\n",
        "        results['accuracy_gap'] = max(group_accuracies.values()) - min(group_accuracies.values())\n",
        "    return results\n",
        "\n",
        "def plot_fairness_results(results):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    bars = plt.bar(results['group_accuracies'].keys(), results['group_accuracies'].values(), color=sns.color_palette('viridis', len(results['group_accuracies'])))\n",
        "    plt.title('Harvard-GF Baseline Model Accuracy by Race', fontsize=16)\n",
        "    plt.xlabel('Race'); plt.ylabel('Accuracy'); plt.ylim(0, 1.0)\n",
        "    plt.axhline(y=results['overall_acc'], color='r', linestyle='--', label=f\"Overall Acc: {results['overall_acc']:.3f}\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(RESULTS_DIR, 'harvardgf_baseline_fairness_evaluation.png'))\n",
        "    plt.show()\n",
        "\n",
        "# === 평가 실행 ===\n",
        "if dataloaders:\n",
        "    model_to_evaluate = get_model(num_classes=len(label_maps[TARGET_ATTR]), pretrained=False)\n",
        "    try:\n",
        "        model_to_evaluate.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
        "        baseline_results = evaluate_fairness(model_to_evaluate, dataloaders['val'], nn.CrossEntropyLoss())\n",
        "        print(\"\\n--- Harvard-GF Baseline Model Evaluation Results ---\")\n",
        "        print(f\"Overall Accuracy: {baseline_results['overall_acc']:.4f}\")\n",
        "        print(\"\\nGroup Accuracies:\")\n",
        "        for group, acc in baseline_results['group_accuracies'].items():\n",
        "            print(f\"  - {group}: {acc:.4f} (n={baseline_results['group_sizes'][group]})\")\n",
        "        print(f\"\\nMaximum Accuracy Gap: {baseline_results.get('accuracy_gap', 0):.4f}\")\n",
        "        plot_fairness_results(baseline_results)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Saved model not found. Please run the training cell first.\")\n",
        "else:\n",
        "    print(\"Dataloaders not available.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 단계 3: Micro-Bias Sensitivity Curve (편향 민감도 곡선) 분석\n",
        "\n",
        "데이터셋의 인종 비율 변화에 모델의 진단 성능이 얼마나 민감하게 반응하는지 측정합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 3.1 편향 민감도 곡선 분석 ===\n",
        "\n",
        "# TODO: BIAS_INJECTION_GROUP을 Harvard-GF 데이터셋에 존재하는 인종 그룹명으로 수정해야 합니다.\n",
        "BIAS_INJECTION_GROUP = 'White' # 예시\n",
        "BIAS_STEPS = np.arange(0.2, 0.55, 0.05)\n",
        "\n",
        "class HGFBiasedSampler(Sampler):\n",
        "    def __init__(self, dataset, target_group_name, desired_group_ratio):\n",
        "        self.dataset = dataset\n",
        "        full_df = pd.read_csv(LABEL_FILE).iloc[dataset.indices]\n",
        "        target_mask = (full_df[SENSITIVE_ATTR] == target_group_name)\n",
        "        self.target_indices = full_df[target_mask].index.tolist()\n",
        "        self.other_indices = full_df[~target_mask].index.tolist()\n",
        "        self.num_target_samples = int(len(dataset) * desired_group_ratio)\n",
        "        self.num_other_samples = len(dataset) - self.num_target_samples\n",
        "\n",
        "    def __iter__(self):\n",
        "        target_samples = np.random.choice(self.target_indices, self.num_target_samples, replace=True)\n",
        "        other_samples = np.random.choice(self.other_indices, self.num_other_samples, replace=True)\n",
        "        final_indices = np.concatenate([target_samples, other_samples])\n",
        "        np.random.shuffle(final_indices)\n",
        "        return iter(final_indices)\n",
        "    def __len__(self): return len(self.dataset)\n",
        "\n",
        "def run_sensitivity_analysis(model, dataset):\n",
        "    results_list = []\n",
        "    model.eval()\n",
        "    for ratio in tqdm(BIAS_STEPS, desc=\"Analyzing Bias Sensitivity\"):\n",
        "        sampler = HGFBiasedSampler(dataset, BIAS_INJECTION_GROUP, ratio)\n",
        "        # 중요: Sampler가 전체 데이터셋의 인덱스를 반환하므로, DataLoader의 dataset도 full_dataset을 가리키도록 해야함.\n",
        "        # 이 경우, Subset이 아닌 원본 Dataset 클래스에 인덱스를 전달하는 방식으로 재구성 필요.\n",
        "        # 여기서는 편의상 매번 데이터셋을 다시 필터링하여 생성합니다.\n",
        "        \n",
        "        # Sampler가 반환하는 인덱스로 새로운 Subset 생성\n",
        "        sampled_indices = list(iter(sampler))\n",
        "        sampled_subset = torch.utils.data.Subset(dataset.dataset, sampled_indices)\n",
        "        dataloader = DataLoader(sampled_subset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
        "\n",
        "        eval_results = evaluate_fairness(model, dataloader, nn.CrossEntropyLoss())\n",
        "        eval_results['bias_ratio'] = ratio\n",
        "        results_list.append(eval_results)\n",
        "    return pd.DataFrame(results_list)\n",
        "\n",
        "# --- 분석 실행 및 시각화 ---\n",
        "if 'baseline_model' in locals():\n",
        "    sensitivity_results_df = run_sensitivity_analysis(baseline_model, image_datasets['val'])\n",
        "    \n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 7))\n",
        "    fig.suptitle(f\"Harvard-GF - Micro-Bias Sensitivity Curve\", fontsize=16)\n",
        "    ax1.plot(sensitivity_results_df['bias_ratio'], sensitivity_results_df['overall_acc'], marker='o', label='Overall Acc')\n",
        "    ax1.set_xlabel(f\"Proportion of '{BIAS_INJECTION_GROUP}' samples\"); ax1.set_ylabel('Accuracy')\n",
        "    ax1.set_title('Accuracy vs. Bias'); ax1.legend()\n",
        "    ax2.plot(sensitivity_results_df['bias_ratio'], sensitivity_results_df['accuracy_gap'], marker='o', color='r', label='Max Acc Gap')\n",
        "    ax2.set_xlabel(f\"Proportion of '{BIAS_INJECTION_GROUP}' samples\"); ax2.set_ylabel('Max Accuracy Gap')\n",
        "    ax2.set_title('Fairness vs. Bias'); ax2.legend()\n",
        "    plt.savefig(os.path.join(RESULTS_DIR, 'harvardgf_micro_bias_sensitivity_curve.png'))\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Baseline model not available.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 단계 4: Over-Correction Damage Index (ODI, 과보정 피해 지수) 분석\n",
        "\n",
        "Reweighing 기법 적용의 효율성을 평가하기 위해 ODI를 계산합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 4.1 ODI 계산 ===\n",
        "\n",
        "if 'baseline_results' in locals():\n",
        "    # --- 1. Reweighing 가중치 계산 ---\n",
        "    train_df = pd.read_csv(LABEL_FILE).iloc[image_datasets['train'].indices]\n",
        "    group_counts = train_df[SENSITIVE_ATTR].value_counts()\n",
        "    group_weights = {group: len(train_df) / (len(group_counts) * count) for group, count in group_counts.items()}\n",
        "    sample_weights = train_df[SENSITIVE_ATTR].apply(lambda g: group_weights[g]).to_numpy()\n",
        "    sampler = WeightedRandomSampler(torch.from_numpy(sample_weights).double(), len(sample_weights))\n",
        "    reweighed_loader = DataLoader(image_datasets['train'], batch_size=BATCH_SIZE, sampler=sampler, num_workers=NUM_WORKERS)\n",
        "    reweighed_dataloaders = {'train': reweighed_loader, 'val': dataloaders['val']}\n",
        "\n",
        "    # --- 2. 모델 재학습 ---\n",
        "    reweighed_model = get_model(num_classes=len(label_maps[TARGET_ATTR]), pretrained=True)\n",
        "    optimizer = optim.SGD(reweighed_model.fc.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
        "    reweighed_model, _ = train_model(reweighed_model, nn.CrossEntropyLoss(), optimizer, reweighed_dataloaders, num_epochs=NUM_EPOCHS)\n",
        "\n",
        "    # --- 3. ODI 계산 ---\n",
        "    reweighed_results = evaluate_fairness(reweighed_model, dataloaders['val'], nn.CrossEntropyLoss())\n",
        "    accuracy_drop = baseline_results['overall_acc'] - reweighed_results['overall_acc']\n",
        "    fairness_gain = baseline_results['accuracy_gap'] - reweighed_results['accuracy_gap']\n",
        "    odi = accuracy_drop / fairness_gain if fairness_gain > 1e-6 else float('inf')\n",
        "    \n",
        "    print(\"\\n--- Over-Correction Damage Index (ODI) ---\")\n",
        "    print(f\"Accuracy Drop: {accuracy_drop:.4f}\")\n",
        "    print(f\"Fairness Gain (Gap Reduction): {fairness_gain:.4f}\")\n",
        "    print(f\"ODI (Accuracy Drop / Fairness Gain): {odi:.4f}\")\n",
        "else:\n",
        "    print(\"Baseline model results not available.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 단계 5: Hidden Subgroup Discovery (잠복 하위집단 탐지)\n",
        "\n",
        "단일 속성(`race`) 분석에서는 드러나지 않는, 여러 속성이 교차하는 특정 하위집단(예: '특정 인종의 특정 성별')에서의 잠재적 편향을 탐지합니다.\n",
        "\n",
        "**참고**: 이 분석을 위해서는 라벨 파일에 `race` 외에 `gender`와 같은 추가적인 민감 속성 컬럼이 필요합니다. 아래 코드는 해당 컬럼이 존재한다고 가정하고 작성된 예시입니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 5.1 잠복 하위집단 분석 ===\n",
        "# TODO: 이 셀을 실행하려면 Harvard-GF 라벨 파일에 'gender'와 같은 추가 속성 컬럼이 필요합니다.\n",
        "\n",
        "ADDITIONAL_ATTR = 'gender' # 분석에 사용할 추가 속성\n",
        "\n",
        "def get_predictions_for_subgroup_analysis(model, dataloader):\n",
        "    # 이 함수는 데이터셋 클래스에 추가 속성 처리가 구현되어 있다고 가정합니다.\n",
        "    # (현재 HarvardGFDataset에는 'gender' 처리가 미구현)\n",
        "    # 아래는 개념 증명을 위한 Pseudo-code에 가깝습니다.\n",
        "    pass # 실제 구현 필요\n",
        "\n",
        "if 'baseline_model' in locals() and ADDITIONAL_ATTR in pd.read_csv(LABEL_FILE).columns:\n",
        "    # predictions_df = get_predictions_for_subgroup_analysis(baseline_model, dataloaders['val'])\n",
        "    # subgroup_analysis = predictions_df.groupby([SENSITIVE_ATTR, ADDITIONAL_ATTR]).agg(...)\n",
        "    # ... 시각화 코드 ...\n",
        "    print(\"Hidden subgroup analysis code would run here.\")\n",
        "    print(\"Please implement the data handling for the additional attribute first.\")\n",
        "else:\n",
        "    print(f\"'{ADDITIONAL_ATTR}' column not found in the label file or baseline model is not ready.\")\n",
        "    print(\"Skipping hidden subgroup discovery.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
